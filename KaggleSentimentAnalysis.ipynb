{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KaggleSentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9FEONwSMjKh",
        "colab_type": "code",
        "outputId": "5b59ac7b-2d55-4864-b8b0-a705ad0f6e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Input, Flatten\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x_ff2EUMqgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wordprep(report):\n",
        "    puncs = \"\".join(string.punctuation)\n",
        "    tokens = report.split(' ')\n",
        "    words = []\n",
        "    for token in tokens:\n",
        "        for punc in puncs:\n",
        "            while punc in token:\n",
        "                token = token.replace(punc, \"\")\n",
        "        if (token != \" \"):\n",
        "            words.append(token)\n",
        "    stop_words = list(set(stopwords.words('english')))\n",
        "    out = [x for x in words if x.lower() not in stop_words]\n",
        "    df_out = \" \".join(out)\n",
        "    \n",
        "    return ' '.join(df_out.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8DWDjY0NPb_",
        "colab_type": "code",
        "outputId": "04a3a002-6fd1-412b-e8d0-3f0f4c8c380a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bv0Ps0YNUbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(\"/content/drive/My Drive/kaggle_sentiment_data/train.tsv\",delimiter=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnC_LPrJU5J7",
        "colab_type": "code",
        "outputId": "1529f97c-fe59-4d08-d466-082ba0f2b84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df_train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156055</th>\n",
              "      <td>156056</td>\n",
              "      <td>8544</td>\n",
              "      <td>Hearst 's</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156056</th>\n",
              "      <td>156057</td>\n",
              "      <td>8544</td>\n",
              "      <td>forced avuncular chortles</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156057</th>\n",
              "      <td>156058</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular chortles</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156058</th>\n",
              "      <td>156059</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156059</th>\n",
              "      <td>156060</td>\n",
              "      <td>8544</td>\n",
              "      <td>chortles</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>156060 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        PhraseId  ...  Sentiment\n",
              "0              1  ...          1\n",
              "1              2  ...          2\n",
              "2              3  ...          2\n",
              "3              4  ...          2\n",
              "4              5  ...          2\n",
              "...          ...  ...        ...\n",
              "156055    156056  ...          2\n",
              "156056    156057  ...          1\n",
              "156057    156058  ...          3\n",
              "156058    156059  ...          2\n",
              "156059    156060  ...          2\n",
              "\n",
              "[156060 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x_rmpmJTWGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv(\"/content/drive/My Drive/kaggle_sentiment_data/test.tsv\",delimiter=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UANWdan9VLCa",
        "colab_type": "code",
        "outputId": "7e830664-6c4d-4ad9-df21-2a4bcc11e13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>156061</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156062</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>156063</td>\n",
              "      <td>8545</td>\n",
              "      <td>An</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>156064</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine effort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156065</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66287</th>\n",
              "      <td>222348</td>\n",
              "      <td>11855</td>\n",
              "      <td>A long-winded , predictable scenario .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66288</th>\n",
              "      <td>222349</td>\n",
              "      <td>11855</td>\n",
              "      <td>A long-winded , predictable scenario</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66289</th>\n",
              "      <td>222350</td>\n",
              "      <td>11855</td>\n",
              "      <td>A long-winded ,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66290</th>\n",
              "      <td>222351</td>\n",
              "      <td>11855</td>\n",
              "      <td>A long-winded</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66291</th>\n",
              "      <td>222352</td>\n",
              "      <td>11855</td>\n",
              "      <td>predictable scenario</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>66292 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       PhraseId  SentenceId                                             Phrase\n",
              "0        156061        8545  An intermittently pleasing but mostly routine ...\n",
              "1        156062        8545  An intermittently pleasing but mostly routine ...\n",
              "2        156063        8545                                                 An\n",
              "3        156064        8545  intermittently pleasing but mostly routine effort\n",
              "4        156065        8545         intermittently pleasing but mostly routine\n",
              "...         ...         ...                                                ...\n",
              "66287    222348       11855             A long-winded , predictable scenario .\n",
              "66288    222349       11855               A long-winded , predictable scenario\n",
              "66289    222350       11855                                    A long-winded ,\n",
              "66290    222351       11855                                      A long-winded\n",
              "66291    222352       11855                               predictable scenario\n",
              "\n",
              "[66292 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFIBNzCITlJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train[\"cleansed_txt\"] = df_train[\"Phrase\"].apply(lambda x: wordprep(x))\n",
        "df_test[\"cleansed_txt\"] = df_test[\"Phrase\"].apply(lambda x: wordprep(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mAtThr0n_pY",
        "colab_type": "code",
        "outputId": "3ef15738-c2e9-40f0-eff1-df24a80e4996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df_train"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>cleansed_txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapades demonstrating adage good goos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "      <td>series escapades demonstrating adage good goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156055</th>\n",
              "      <td>156056</td>\n",
              "      <td>8544</td>\n",
              "      <td>Hearst 's</td>\n",
              "      <td>2</td>\n",
              "      <td>Hearst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156056</th>\n",
              "      <td>156057</td>\n",
              "      <td>8544</td>\n",
              "      <td>forced avuncular chortles</td>\n",
              "      <td>1</td>\n",
              "      <td>forced avuncular chortles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156057</th>\n",
              "      <td>156058</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular chortles</td>\n",
              "      <td>3</td>\n",
              "      <td>avuncular chortles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156058</th>\n",
              "      <td>156059</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular</td>\n",
              "      <td>2</td>\n",
              "      <td>avuncular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156059</th>\n",
              "      <td>156060</td>\n",
              "      <td>8544</td>\n",
              "      <td>chortles</td>\n",
              "      <td>2</td>\n",
              "      <td>chortles</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>156060 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        PhraseId  ...                                       cleansed_txt\n",
              "0              1  ...  series escapades demonstrating adage good goos...\n",
              "1              2  ...    series escapades demonstrating adage good goose\n",
              "2              3  ...                                             series\n",
              "3              4  ...                                                   \n",
              "4              5  ...                                             series\n",
              "...          ...  ...                                                ...\n",
              "156055    156056  ...                                             Hearst\n",
              "156056    156057  ...                          forced avuncular chortles\n",
              "156057    156058  ...                                 avuncular chortles\n",
              "156058    156059  ...                                          avuncular\n",
              "156059    156060  ...                                           chortles\n",
              "\n",
              "[156060 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu49hUvT6Xez",
        "colab_type": "code",
        "outputId": "5561c30d-70cf-4695-b8c2-00240406c026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "df_train = df_train[df_train.cleansed_txt != '']\n",
        "print(df_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        PhraseId  ...                                       cleansed_txt\n",
            "0              1  ...  series escapades demonstrating adage good goos...\n",
            "1              2  ...    series escapades demonstrating adage good goose\n",
            "2              3  ...                                             series\n",
            "4              5  ...                                             series\n",
            "5              6  ...           escapades demonstrating adage good goose\n",
            "...          ...  ...                                                ...\n",
            "156055    156056  ...                                             Hearst\n",
            "156056    156057  ...                          forced avuncular chortles\n",
            "156057    156058  ...                                 avuncular chortles\n",
            "156058    156059  ...                                          avuncular\n",
            "156059    156060  ...                                           chortles\n",
            "\n",
            "[154906 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL_RzGPsocP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To change lexicon size\n",
        "nword1 = 128\n",
        "nword2 = 256\n",
        "nword3 = 512\n",
        "nwords = nword1\n",
        "tokenizer = Tokenizer(num_words=nwords, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n",
        "tokenizer.fit_on_texts(df_train[\"cleansed_txt\"].values)\n",
        "tokenizer.fit_on_texts(df_test[\"cleansed_txt\"].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV7CeIMfg1lA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "a1cf34b0-7716-4231-d869-f4bb3f10949b"
      },
      "source": [
        "df_train"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>cleansed_txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapades demonstrating adage good goos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "      <td>series escapades demonstrating adage good goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>of escapades demonstrating the adage that what...</td>\n",
              "      <td>2</td>\n",
              "      <td>escapades demonstrating adage good goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156055</th>\n",
              "      <td>156056</td>\n",
              "      <td>8544</td>\n",
              "      <td>Hearst 's</td>\n",
              "      <td>2</td>\n",
              "      <td>Hearst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156056</th>\n",
              "      <td>156057</td>\n",
              "      <td>8544</td>\n",
              "      <td>forced avuncular chortles</td>\n",
              "      <td>1</td>\n",
              "      <td>forced avuncular chortles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156057</th>\n",
              "      <td>156058</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular chortles</td>\n",
              "      <td>3</td>\n",
              "      <td>avuncular chortles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156058</th>\n",
              "      <td>156059</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular</td>\n",
              "      <td>2</td>\n",
              "      <td>avuncular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156059</th>\n",
              "      <td>156060</td>\n",
              "      <td>8544</td>\n",
              "      <td>chortles</td>\n",
              "      <td>2</td>\n",
              "      <td>chortles</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>154906 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        PhraseId  ...                                       cleansed_txt\n",
              "0              1  ...  series escapades demonstrating adage good goos...\n",
              "1              2  ...    series escapades demonstrating adage good goose\n",
              "2              3  ...                                             series\n",
              "4              5  ...                                             series\n",
              "5              6  ...           escapades demonstrating adage good goose\n",
              "...          ...  ...                                                ...\n",
              "156055    156056  ...                                             Hearst\n",
              "156056    156057  ...                          forced avuncular chortles\n",
              "156057    156058  ...                                 avuncular chortles\n",
              "156058    156059  ...                                          avuncular\n",
              "156059    156060  ...                                           chortles\n",
              "\n",
              "[154906 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XhXH_sacn5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = tokenizer.texts_to_sequences(df_train[\"cleansed_txt\"].values)\n",
        "X_test = tokenizer.texts_to_sequences(df_test[\"cleansed_txt\"].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVkrk57xmBY2",
        "colab_type": "code",
        "outputId": "73e2ca10-9020-4965-e7b3-c2e02e907073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 88, 8, 10, 6],\n",
              " [8],\n",
              " [],\n",
              " [],\n",
              " [8],\n",
              " [8],\n",
              " [],\n",
              " [8],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [8],\n",
              " [8],\n",
              " [8],\n",
              " [8],\n",
              " [8],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [88, 8, 10, 6],\n",
              " [88, 8, 10, 6],\n",
              " [88],\n",
              " [88],\n",
              " [8, 10, 6],\n",
              " [10, 6],\n",
              " [10, 6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [10, 6],\n",
              " [10, 6],\n",
              " [],\n",
              " [10, 6],\n",
              " [],\n",
              " [10, 6],\n",
              " [10, 6],\n",
              " [],\n",
              " [10, 6],\n",
              " [10, 6],\n",
              " [10, 6],\n",
              " [],\n",
              " [10, 6],\n",
              " [10, 6],\n",
              " [10],\n",
              " [6],\n",
              " [6],\n",
              " [6],\n",
              " [108],\n",
              " [108],\n",
              " [108],\n",
              " [108],\n",
              " [],\n",
              " [108],\n",
              " [108],\n",
              " [],\n",
              " [],\n",
              " [108],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [14, 26, 34, 116, 12, 4],\n",
              " [14, 26],\n",
              " [14],\n",
              " [14],\n",
              " [],\n",
              " [26],\n",
              " [26],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [26],\n",
              " [34, 116, 12, 4],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [34, 116, 12, 4],\n",
              " [34, 116, 12, 4],\n",
              " [34],\n",
              " [116, 12, 4],\n",
              " [116, 12, 4],\n",
              " [116, 12],\n",
              " [116, 12],\n",
              " [116],\n",
              " [12],\n",
              " [4],\n",
              " [],\n",
              " [4],\n",
              " [4],\n",
              " [4],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [80, 40],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [80, 40],\n",
              " [80, 40],\n",
              " [80],\n",
              " [],\n",
              " [80],\n",
              " [80],\n",
              " [80],\n",
              " [],\n",
              " [80],\n",
              " [40],\n",
              " [40],\n",
              " [40],\n",
              " [40],\n",
              " [],\n",
              " [40],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [50],\n",
              " [],\n",
              " [50],\n",
              " [50],\n",
              " [50],\n",
              " [],\n",
              " [50],\n",
              " [50],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [5],\n",
              " [],\n",
              " [],\n",
              " [5],\n",
              " [5],\n",
              " [5],\n",
              " [],\n",
              " [],\n",
              " [5],\n",
              " [5],\n",
              " [],\n",
              " [5],\n",
              " [5],\n",
              " [],\n",
              " [5],\n",
              " [5],\n",
              " [],\n",
              " [5],\n",
              " [5],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [3, 10],\n",
              " [3, 10],\n",
              " [3, 10],\n",
              " [3, 10],\n",
              " [3],\n",
              " [3],\n",
              " [10],\n",
              " [],\n",
              " [],\n",
              " [10],\n",
              " [37],\n",
              " [37],\n",
              " [37],\n",
              " [37],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [15, 4, 110],\n",
              " [15, 4, 110],\n",
              " [15, 4, 110],\n",
              " [15],\n",
              " [15],\n",
              " [15],\n",
              " [15],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [4, 110],\n",
              " [],\n",
              " [4, 110],\n",
              " [110],\n",
              " [],\n",
              " [110],\n",
              " [110],\n",
              " [110],\n",
              " [110],\n",
              " [],\n",
              " [110],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [51],\n",
              " [51],\n",
              " [51],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [28, 20, 8, 56],\n",
              " [],\n",
              " [28, 20, 8, 56],\n",
              " [28, 20, 8, 56],\n",
              " [28, 20, 8, 56],\n",
              " [28],\n",
              " [20, 8, 56],\n",
              " [],\n",
              " [20, 8, 56],\n",
              " [20, 8, 56],\n",
              " [],\n",
              " [20, 8, 56],\n",
              " [20, 8, 56],\n",
              " [],\n",
              " [],\n",
              " [20, 8, 56],\n",
              " [20, 8, 56],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [20, 8, 56],\n",
              " [20, 8, 56],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [20, 8, 56],\n",
              " [20],\n",
              " [],\n",
              " [20],\n",
              " [8, 56],\n",
              " [8, 56],\n",
              " [],\n",
              " [],\n",
              " [8, 56],\n",
              " [8, 56],\n",
              " [56],\n",
              " [65, 5, 53],\n",
              " [65, 5, 53],\n",
              " [65],\n",
              " [5, 53],\n",
              " [5, 53],\n",
              " [53],\n",
              " [53],\n",
              " [],\n",
              " [53],\n",
              " [],\n",
              " [53],\n",
              " [53],\n",
              " [53],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [22, 116, 4, 6],\n",
              " [22, 116],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [22, 116],\n",
              " [22, 116],\n",
              " [22],\n",
              " [],\n",
              " [22],\n",
              " [116],\n",
              " [116],\n",
              " [],\n",
              " [116],\n",
              " [116],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [4, 6],\n",
              " [4, 6],\n",
              " [6],\n",
              " [6],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [5],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [5],\n",
              " [5],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [5, 76, 24, 10, 2],\n",
              " [5, 76],\n",
              " [76],\n",
              " [76],\n",
              " [76],\n",
              " [76],\n",
              " [76],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [24, 10, 2],\n",
              " [],\n",
              " [24, 10, 2],\n",
              " [24, 10, 2],\n",
              " [24, 10, 2],\n",
              " [24, 10, 2],\n",
              " [24, 10, 2],\n",
              " [],\n",
              " [],\n",
              " [24, 10, 2],\n",
              " [24, 10, 2],\n",
              " [24, 10],\n",
              " [24],\n",
              " [24],\n",
              " [10],\n",
              " [2],\n",
              " [2],\n",
              " [],\n",
              " [2],\n",
              " [],\n",
              " [2],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [68],\n",
              " [68],\n",
              " [68],\n",
              " [68],\n",
              " [],\n",
              " [68],\n",
              " [68],\n",
              " [68],\n",
              " [68],\n",
              " [],\n",
              " [],\n",
              " [68],\n",
              " [68],\n",
              " [29, 125, 75],\n",
              " [29],\n",
              " [125, 75],\n",
              " [125, 75],\n",
              " [125],\n",
              " [125],\n",
              " [],\n",
              " [125],\n",
              " [125],\n",
              " [125],\n",
              " [75],\n",
              " [75],\n",
              " [17, 56, 2, 44, 2],\n",
              " [17, 56, 2, 44, 2],\n",
              " [17],\n",
              " [17],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [56, 2, 44, 2],\n",
              " [56],\n",
              " [56],\n",
              " [2, 44, 2],\n",
              " [2],\n",
              " [44, 2],\n",
              " [44],\n",
              " [44],\n",
              " [],\n",
              " [44],\n",
              " [],\n",
              " [],\n",
              " [44],\n",
              " [44],\n",
              " [],\n",
              " [],\n",
              " [44],\n",
              " [44],\n",
              " [44],\n",
              " [],\n",
              " [44],\n",
              " [2],\n",
              " [],\n",
              " [2],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [2],\n",
              " [2],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [2],\n",
              " [2],\n",
              " [2],\n",
              " [2],\n",
              " [2],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [5, 121, 106],\n",
              " [5, 121, 106],\n",
              " [5, 121, 106],\n",
              " [5, 121, 106],\n",
              " [121, 106],\n",
              " [121],\n",
              " [121],\n",
              " [121],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [106],\n",
              " [106],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [107, 8, 1],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [107, 8, 1],\n",
              " [107, 8, 1],\n",
              " [],\n",
              " [107, 8, 1],\n",
              " [107],\n",
              " [107],\n",
              " [107],\n",
              " [],\n",
              " [107],\n",
              " [],\n",
              " [],\n",
              " [8, 1],\n",
              " [8, 1],\n",
              " [8],\n",
              " [8],\n",
              " [],\n",
              " [1],\n",
              " [1],\n",
              " [1],\n",
              " [1],\n",
              " [],\n",
              " [1],\n",
              " [1],\n",
              " [45, 114, 13],\n",
              " [45],\n",
              " [45],\n",
              " [114, 13],\n",
              " [114, 13],\n",
              " [114, 13],\n",
              " [114, 13],\n",
              " [114, 13],\n",
              " [114],\n",
              " [13],\n",
              " [13],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [99, 45],\n",
              " [99, 45],\n",
              " [99, 45],\n",
              " [],\n",
              " [99, 45],\n",
              " [99, 45],\n",
              " [99],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [99],\n",
              " [45],\n",
              " [],\n",
              " [45],\n",
              " [],\n",
              " [1],\n",
              " [1],\n",
              " [1],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [3, 20, 120, 51],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [3, 20, 120, 51],\n",
              " [3, 20, 120, 51],\n",
              " [3, 20, 120, 51],\n",
              " [3, 20, 120, 51],\n",
              " [20, 120, 51],\n",
              " [20],\n",
              " [],\n",
              " [120, 51],\n",
              " [120, 51],\n",
              " [120, 51],\n",
              " [120, 51],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [120, 51],\n",
              " [],\n",
              " [120, 51],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [120, 51],\n",
              " [120],\n",
              " [120],\n",
              " [120],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [84],\n",
              " [84],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [84],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [84],\n",
              " [84],\n",
              " [84],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [84],\n",
              " [84],\n",
              " [68, 41],\n",
              " [41],\n",
              " [41],\n",
              " [41],\n",
              " [41],\n",
              " [41],\n",
              " [],\n",
              " [41],\n",
              " [],\n",
              " [81, 5],\n",
              " [81, 5],\n",
              " [81],\n",
              " [81],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [81],\n",
              " [81],\n",
              " [81],\n",
              " [],\n",
              " [81],\n",
              " [5],\n",
              " [5],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [5],\n",
              " [],\n",
              " [],\n",
              " [5],\n",
              " [],\n",
              " [5],\n",
              " [],\n",
              " [61, 73, 5, 22, 33],\n",
              " [61, 73, 5, 22, 33],\n",
              " [61, 73, 5, 22],\n",
              " [61, 73, 5, 22],\n",
              " [61, 73, 5, 22],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [61, 73, 5, 22],\n",
              " [61, 73, 5, 22],\n",
              " [61],\n",
              " [61],\n",
              " [61],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [73, 5, 22],\n",
              " [73, 5, 22],\n",
              " [73],\n",
              " [5, 22],\n",
              " [22],\n",
              " [33],\n",
              " [33],\n",
              " [33],\n",
              " [33],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [15, 56, 121],\n",
              " [15, 56, 121],\n",
              " [15, 56, 121],\n",
              " [15, 56, 121],\n",
              " [15, 56, 121],\n",
              " [15, 56, 121],\n",
              " [15, 56, 121],\n",
              " [15, 56],\n",
              " [15],\n",
              " [121],\n",
              " [121],\n",
              " [121],\n",
              " [121],\n",
              " [121],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [61, 1, 95, 28],\n",
              " [],\n",
              " [61, 1, 95, 28],\n",
              " [61, 1, 95, 28],\n",
              " [61, 1, 95, 28],\n",
              " [61, 1, 95, 28],\n",
              " [61, 1, 95],\n",
              " [],\n",
              " [61, 1, 95],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [61, 1, 95],\n",
              " [61, 1, 95],\n",
              " [61, 1, 95],\n",
              " [61, 1],\n",
              " [61, 1],\n",
              " [61, 1],\n",
              " [95],\n",
              " [95],\n",
              " [95],\n",
              " [],\n",
              " [28],\n",
              " [28],\n",
              " [],\n",
              " [22],\n",
              " [22],\n",
              " [22],\n",
              " [22],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [22],\n",
              " [22],\n",
              " [22],\n",
              " [22],\n",
              " [],\n",
              " [22],\n",
              " [],\n",
              " [22],\n",
              " [22],\n",
              " [],\n",
              " [],\n",
              " [22],\n",
              " [22],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [99, 2, 108, 20],\n",
              " [99, 2, 108, 20],\n",
              " [2, 108, 20],\n",
              " [2, 108, 20],\n",
              " [],\n",
              " [2, 108, 20],\n",
              " [2, 108, 20],\n",
              " [2, 108, 20],\n",
              " [108, 20],\n",
              " [108, 20],\n",
              " [108],\n",
              " [108],\n",
              " [],\n",
              " [108],\n",
              " [108],\n",
              " [20],\n",
              " [20],\n",
              " [20],\n",
              " [],\n",
              " [2, 23, 81],\n",
              " [2],\n",
              " [2],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [23, 81],\n",
              " [23, 81],\n",
              " [23, 81],\n",
              " [23],\n",
              " [],\n",
              " [23],\n",
              " [23],\n",
              " [],\n",
              " [81],\n",
              " [81],\n",
              " [81],\n",
              " [],\n",
              " [81],\n",
              " [81],\n",
              " [],\n",
              " [81],\n",
              " [],\n",
              " [3, 4, 6, 121],\n",
              " [3, 4, 6, 121],\n",
              " [3, 4],\n",
              " [3, 4],\n",
              " [3],\n",
              " [4],\n",
              " [4],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6, 121],\n",
              " [6],\n",
              " [6],\n",
              " [6],\n",
              " [6],\n",
              " [6],\n",
              " [],\n",
              " [6],\n",
              " [6],\n",
              " [],\n",
              " [121],\n",
              " [121],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q34bwLBzo4se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#min_size = min([len(a) for a in X_train])\n",
        "min_size=5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzCkeUL0o72b",
        "colab_type": "code",
        "outputId": "e3e9157f-7b04-47fd-9723-5116fa736044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "min_size"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDHtPrEDWF2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X_train = pad_sequences(X_train, padding=\"post\")\n",
        "X_test = pad_sequences(X_test, padding=\"post\")\n",
        "X_train = X_train[:, range(min_size*1)]\n",
        "X_test = X_test[:, range(min_size*1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM7amwFti19z",
        "colab_type": "code",
        "outputId": "10f4f709-2ddc-4f62-8871-c9b7ae78b379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8, 88,  8, 10,  6],\n",
              "       [ 8,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KPXReXni-0C",
        "colab_type": "code",
        "outputId": "6d10d065-e7a2-43ab-be0d-f696931c7645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test[1]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ka1gGUiWby9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open(\"/content/drive/My Drive/Thesis/glove.6B.50d.txt\", encoding=\"utf-8\")\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdH9-DYNWhfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((nwords, 50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-l0bD64WnMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1, nwords):\n",
        "    word = tokenizer.index_word[i]\n",
        "    if word in embeddings_index.keys():\n",
        "        embedding_vector = embeddings_index[word]\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Xf53HuWrES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To change batch size\n",
        "embed_dim = 50\n",
        "lstm_out = 8\n",
        "batch_size01 = 8\n",
        "batch_size02 = 16\n",
        "batch_size03 = 32\n",
        "batch_size = batch_size01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2UZ8W-nWuzR",
        "colab_type": "code",
        "outputId": "4c93ab0a-e250-46ac-8b3a-bbfed98b1424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "#opt = optimizers.Adam(lr=1e-4, beta_1=0.8, beta_2=0.9, epsilon=0.2, decay=1e-2)\n",
        "model = Sequential()\n",
        "model.add(Embedding(nwords, embed_dim,input_length = X_train.shape[1], weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(lstm_out,\n",
        "               dropout = 0.3,\n",
        "               recurrent_dropout = 0.3,\n",
        "               kernel_initializer=\"lecun_uniform\",\n",
        "               recurrent_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(5, kernel_initializer=\"random_uniform\", activation=\"softmax\"))\n",
        "adam01 = optimizers.adam(lr=0.1)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\",metrics = [\"accuracy\"])\n",
        "print(model.summary())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 5, 50)             6400      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 8)                 1888      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 45        \n",
            "=================================================================\n",
            "Total params: 8,333\n",
            "Trainable params: 1,933\n",
            "Non-trainable params: 6,400\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDxs6ibPWyWo",
        "colab_type": "code",
        "outputId": "067fc362-4643-4fea-ba62-ce6d18da9443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df_train"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>cleansed_txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapades demonstrating adage good goos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "      <td>series escapades demonstrating adage good goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>series</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>of escapades demonstrating the adage that what...</td>\n",
              "      <td>2</td>\n",
              "      <td>escapades demonstrating adage good goose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156055</th>\n",
              "      <td>156056</td>\n",
              "      <td>8544</td>\n",
              "      <td>Hearst 's</td>\n",
              "      <td>2</td>\n",
              "      <td>Hearst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156056</th>\n",
              "      <td>156057</td>\n",
              "      <td>8544</td>\n",
              "      <td>forced avuncular chortles</td>\n",
              "      <td>1</td>\n",
              "      <td>forced avuncular chortles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156057</th>\n",
              "      <td>156058</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular chortles</td>\n",
              "      <td>3</td>\n",
              "      <td>avuncular chortles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156058</th>\n",
              "      <td>156059</td>\n",
              "      <td>8544</td>\n",
              "      <td>avuncular</td>\n",
              "      <td>2</td>\n",
              "      <td>avuncular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156059</th>\n",
              "      <td>156060</td>\n",
              "      <td>8544</td>\n",
              "      <td>chortles</td>\n",
              "      <td>2</td>\n",
              "      <td>chortles</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>154906 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        PhraseId  ...                                       cleansed_txt\n",
              "0              1  ...  series escapades demonstrating adage good goos...\n",
              "1              2  ...    series escapades demonstrating adage good goose\n",
              "2              3  ...                                             series\n",
              "4              5  ...                                             series\n",
              "5              6  ...           escapades demonstrating adage good goose\n",
              "...          ...  ...                                                ...\n",
              "156055    156056  ...                                             Hearst\n",
              "156056    156057  ...                          forced avuncular chortles\n",
              "156057    156058  ...                                 avuncular chortles\n",
              "156058    156059  ...                                          avuncular\n",
              "156059    156060  ...                                           chortles\n",
              "\n",
              "[154906 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDSB89C0W8lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = pd.get_dummies(df_train[\"Sentiment\"]).values\n",
        "#Y_test = pd.get_dummies(df_test[\"TNM-stage\"]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-IxQ4lHjktE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRhFk7Fujljx",
        "colab_type": "code",
        "outputId": "78f4c24e-0479-43fc-8750-df75b785de33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(154906, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTVWDgZyjp07",
        "colab_type": "code",
        "outputId": "5cdbb2e5-706f-426e-ee04-f018fa8289af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_train.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(154906, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46YxWwAljtJO",
        "colab_type": "code",
        "outputId": "75111dbe-ba55-4d10-e17e-9e42d55262d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66292, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AieOFbNMnFRA",
        "colab_type": "code",
        "outputId": "56a21185-e175-4727-bd61-38179873c18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, Y_train, batch_size =batch_size, epochs = 200, verbose = 1, shuffle=False, callbacks=[es])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.4077 - accuracy: 0.8221\n",
            "Epoch 2/200\n",
            "   432/154906 [..............................] - ETA: 58s - loss: 0.3813 - accuracy: 0.8361 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "154906/154906 [==============================] - 57s 369us/step - loss: 0.4040 - accuracy: 0.8233\n",
            "Epoch 3/200\n",
            "154906/154906 [==============================] - 57s 370us/step - loss: 0.4028 - accuracy: 0.8234\n",
            "Epoch 4/200\n",
            "154906/154906 [==============================] - 58s 373us/step - loss: 0.4018 - accuracy: 0.8236\n",
            "Epoch 5/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4013 - accuracy: 0.8237\n",
            "Epoch 6/200\n",
            "154906/154906 [==============================] - 58s 374us/step - loss: 0.4011 - accuracy: 0.8237\n",
            "Epoch 7/200\n",
            "154906/154906 [==============================] - 58s 372us/step - loss: 0.4009 - accuracy: 0.8237\n",
            "Epoch 8/200\n",
            "154906/154906 [==============================] - 57s 370us/step - loss: 0.4008 - accuracy: 0.8236\n",
            "Epoch 9/200\n",
            "154906/154906 [==============================] - 57s 368us/step - loss: 0.4007 - accuracy: 0.8238\n",
            "Epoch 10/200\n",
            "154906/154906 [==============================] - 58s 371us/step - loss: 0.4006 - accuracy: 0.8239\n",
            "Epoch 11/200\n",
            "154906/154906 [==============================] - 57s 371us/step - loss: 0.4006 - accuracy: 0.8237\n",
            "Epoch 12/200\n",
            "154906/154906 [==============================] - 57s 369us/step - loss: 0.4005 - accuracy: 0.8238\n",
            "Epoch 13/200\n",
            "154906/154906 [==============================] - 57s 369us/step - loss: 0.4007 - accuracy: 0.8237\n",
            "Epoch 14/200\n",
            "154906/154906 [==============================] - 57s 367us/step - loss: 0.4006 - accuracy: 0.8238\n",
            "Epoch 15/200\n",
            "154906/154906 [==============================] - 57s 366us/step - loss: 0.4006 - accuracy: 0.8236\n",
            "Epoch 16/200\n",
            "154906/154906 [==============================] - 57s 370us/step - loss: 0.4003 - accuracy: 0.8237\n",
            "Epoch 17/200\n",
            "154906/154906 [==============================] - 56s 363us/step - loss: 0.4005 - accuracy: 0.8238\n",
            "Epoch 18/200\n",
            "154906/154906 [==============================] - 56s 362us/step - loss: 0.4004 - accuracy: 0.8239\n",
            "Epoch 19/200\n",
            "154906/154906 [==============================] - 56s 364us/step - loss: 0.4004 - accuracy: 0.8238\n",
            "Epoch 20/200\n",
            "154906/154906 [==============================] - 57s 367us/step - loss: 0.4003 - accuracy: 0.8237\n",
            "Epoch 21/200\n",
            "154906/154906 [==============================] - 57s 370us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 22/200\n",
            "154906/154906 [==============================] - 57s 369us/step - loss: 0.4003 - accuracy: 0.8239\n",
            "Epoch 23/200\n",
            "154906/154906 [==============================] - 57s 365us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 24/200\n",
            "154906/154906 [==============================] - 57s 365us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 25/200\n",
            "154906/154906 [==============================] - 57s 366us/step - loss: 0.4004 - accuracy: 0.8237\n",
            "Epoch 26/200\n",
            "154906/154906 [==============================] - 56s 364us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 27/200\n",
            "154906/154906 [==============================] - 57s 370us/step - loss: 0.4003 - accuracy: 0.8240\n",
            "Epoch 28/200\n",
            "154906/154906 [==============================] - 56s 363us/step - loss: 0.4003 - accuracy: 0.8237\n",
            "Epoch 29/200\n",
            "154906/154906 [==============================] - 56s 365us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 30/200\n",
            "154906/154906 [==============================] - 57s 365us/step - loss: 0.4003 - accuracy: 0.8236\n",
            "Epoch 31/200\n",
            "154906/154906 [==============================] - 57s 366us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 32/200\n",
            "154906/154906 [==============================] - 57s 369us/step - loss: 0.4004 - accuracy: 0.8239\n",
            "Epoch 33/200\n",
            "154906/154906 [==============================] - 56s 363us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 34/200\n",
            "154906/154906 [==============================] - 57s 367us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 35/200\n",
            "154906/154906 [==============================] - 57s 365us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 36/200\n",
            "154906/154906 [==============================] - 57s 365us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 37/200\n",
            "154906/154906 [==============================] - 57s 365us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 38/200\n",
            "154906/154906 [==============================] - 57s 369us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 39/200\n",
            "154906/154906 [==============================] - 56s 364us/step - loss: 0.4003 - accuracy: 0.8239\n",
            "Epoch 40/200\n",
            "154906/154906 [==============================] - 57s 365us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 41/200\n",
            "154906/154906 [==============================] - 57s 366us/step - loss: 0.4003 - accuracy: 0.8240\n",
            "Epoch 42/200\n",
            "154906/154906 [==============================] - 57s 367us/step - loss: 0.4002 - accuracy: 0.8240\n",
            "Epoch 43/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4002 - accuracy: 0.8240\n",
            "Epoch 44/200\n",
            "154906/154906 [==============================] - 58s 373us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 45/200\n",
            "154906/154906 [==============================] - 58s 372us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 46/200\n",
            "154906/154906 [==============================] - 58s 372us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 47/200\n",
            "154906/154906 [==============================] - 57s 371us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 48/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 49/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 50/200\n",
            "154906/154906 [==============================] - 58s 372us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 51/200\n",
            "154906/154906 [==============================] - 60s 388us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 52/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 53/200\n",
            "154906/154906 [==============================] - 58s 374us/step - loss: 0.4001 - accuracy: 0.8240\n",
            "Epoch 54/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 55/200\n",
            "154906/154906 [==============================] - 58s 374us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 56/200\n",
            "154906/154906 [==============================] - 58s 372us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 57/200\n",
            "154906/154906 [==============================] - 58s 373us/step - loss: 0.4003 - accuracy: 0.8239\n",
            "Epoch 58/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.4002 - accuracy: 0.8240\n",
            "Epoch 59/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.3999 - accuracy: 0.8238\n",
            "Epoch 60/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 61/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 62/200\n",
            "154906/154906 [==============================] - 61s 397us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 63/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 64/200\n",
            "154906/154906 [==============================] - 59s 381us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 65/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 66/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 67/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 68/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4002 - accuracy: 0.8240\n",
            "Epoch 69/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4001 - accuracy: 0.8240\n",
            "Epoch 70/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 71/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4002 - accuracy: 0.8241\n",
            "Epoch 72/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4003 - accuracy: 0.8239\n",
            "Epoch 73/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4001 - accuracy: 0.8240\n",
            "Epoch 74/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4001 - accuracy: 0.8240\n",
            "Epoch 75/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.4001 - accuracy: 0.8241\n",
            "Epoch 76/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 77/200\n",
            "154906/154906 [==============================] - 58s 378us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 78/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 79/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4001 - accuracy: 0.8242\n",
            "Epoch 80/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4001 - accuracy: 0.8241\n",
            "Epoch 81/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 82/200\n",
            "154906/154906 [==============================] - 59s 381us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 83/200\n",
            "154906/154906 [==============================] - 59s 384us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 84/200\n",
            "154906/154906 [==============================] - 59s 381us/step - loss: 0.4001 - accuracy: 0.8240\n",
            "Epoch 85/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4001 - accuracy: 0.8241\n",
            "Epoch 86/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 87/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.3999 - accuracy: 0.8239\n",
            "Epoch 88/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 89/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 90/200\n",
            "154906/154906 [==============================] - 59s 381us/step - loss: 0.3999 - accuracy: 0.8239\n",
            "Epoch 91/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.3999 - accuracy: 0.8239\n",
            "Epoch 92/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.3999 - accuracy: 0.8238\n",
            "Epoch 93/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 94/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 95/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 96/200\n",
            "154906/154906 [==============================] - 59s 384us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 97/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4000 - accuracy: 0.8241\n",
            "Epoch 98/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 99/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 100/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 101/200\n",
            "154906/154906 [==============================] - 60s 385us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 102/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.3999 - accuracy: 0.8239\n",
            "Epoch 103/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 104/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 105/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 106/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 107/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.3999 - accuracy: 0.8239\n",
            "Epoch 108/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.3999 - accuracy: 0.8238\n",
            "Epoch 109/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 110/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 111/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.3999 - accuracy: 0.8239\n",
            "Epoch 112/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 113/200\n",
            "154906/154906 [==============================] - 62s 398us/step - loss: 0.4000 - accuracy: 0.8238\n",
            "Epoch 114/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 115/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 116/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 117/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4000 - accuracy: 0.8237\n",
            "Epoch 118/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.3999 - accuracy: 0.8238\n",
            "Epoch 119/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.3999 - accuracy: 0.8237\n",
            "Epoch 120/200\n",
            "154906/154906 [==============================] - 60s 386us/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 121/200\n",
            "154906/154906 [==============================] - 61s 394us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 122/200\n",
            "154906/154906 [==============================] - 61s 393us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 123/200\n",
            "154906/154906 [==============================] - 63s 407us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 124/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 125/200\n",
            "154906/154906 [==============================] - 60s 385us/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 126/200\n",
            "154906/154906 [==============================] - 59s 384us/step - loss: 0.4000 - accuracy: 0.8238\n",
            "Epoch 127/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.4000 - accuracy: 0.8240\n",
            "Epoch 128/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 129/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 130/200\n",
            "154906/154906 [==============================] - 60s 386us/step - loss: 0.3999 - accuracy: 0.8241\n",
            "Epoch 131/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.3999 - accuracy: 0.8239\n",
            "Epoch 132/200\n",
            "154906/154906 [==============================] - 60s 389us/step - loss: 0.4000 - accuracy: 0.8238\n",
            "Epoch 133/200\n",
            "154906/154906 [==============================] - 61s 394us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 134/200\n",
            "154906/154906 [==============================] - 62s 401us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 135/200\n",
            "154906/154906 [==============================] - 60s 385us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 136/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 137/200\n",
            "154906/154906 [==============================] - 60s 387us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 138/200\n",
            "154906/154906 [==============================] - 60s 386us/step - loss: 0.4001 - accuracy: 0.8237\n",
            "Epoch 139/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 140/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4000 - accuracy: 0.8241\n",
            "Epoch 141/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 142/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 143/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4001 - accuracy: 0.8241\n",
            "Epoch 144/200\n",
            "154906/154906 [==============================] - 61s 395us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 145/200\n",
            "154906/154906 [==============================] - 58s 373us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 146/200\n",
            "154906/154906 [==============================] - 58s 374us/step - loss: 0.3999 - accuracy: 0.8239\n",
            "Epoch 147/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 148/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.4001 - accuracy: 0.8240\n",
            "Epoch 149/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 150/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 151/200\n",
            "154906/154906 [==============================] - 58s 373us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 152/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4000 - accuracy: 0.8237\n",
            "Epoch 153/200\n",
            "154906/154906 [==============================] - 58s 374us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 154/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 155/200\n",
            "154906/154906 [==============================] - 58s 373us/step - loss: 0.4000 - accuracy: 0.8237\n",
            "Epoch 156/200\n",
            "154906/154906 [==============================] - 58s 374us/step - loss: 0.4001 - accuracy: 0.8237\n",
            "Epoch 157/200\n",
            "154906/154906 [==============================] - 58s 374us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 158/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 159/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 160/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4001 - accuracy: 0.8240\n",
            "Epoch 161/200\n",
            "154906/154906 [==============================] - 58s 374us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 162/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 163/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.4001 - accuracy: 0.8237\n",
            "Epoch 164/200\n",
            "154906/154906 [==============================] - 58s 373us/step - loss: 0.4003 - accuracy: 0.8237\n",
            "Epoch 165/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 166/200\n",
            "154906/154906 [==============================] - 60s 389us/step - loss: 0.4001 - accuracy: 0.8237\n",
            "Epoch 167/200\n",
            "154906/154906 [==============================] - 58s 373us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 168/200\n",
            "154906/154906 [==============================] - 58s 375us/step - loss: 0.4001 - accuracy: 0.8237\n",
            "Epoch 169/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 170/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 171/200\n",
            "154906/154906 [==============================] - 58s 376us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 172/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4001 - accuracy: 0.8237\n",
            "Epoch 173/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 174/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 175/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4003 - accuracy: 0.8238\n",
            "Epoch 176/200\n",
            "154906/154906 [==============================] - 61s 396us/step - loss: 0.4002 - accuracy: 0.8236\n",
            "Epoch 177/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4002 - accuracy: 0.8241\n",
            "Epoch 178/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4001 - accuracy: 0.8237\n",
            "Epoch 179/200\n",
            "154906/154906 [==============================] - 58s 377us/step - loss: 0.4002 - accuracy: 0.8237\n",
            "Epoch 180/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 181/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.4000 - accuracy: 0.8239\n",
            "Epoch 182/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4003 - accuracy: 0.8240\n",
            "Epoch 183/200\n",
            "154906/154906 [==============================] - 58s 378us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 184/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 185/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 186/200\n",
            "154906/154906 [==============================] - 59s 381us/step - loss: 0.4002 - accuracy: 0.8237\n",
            "Epoch 187/200\n",
            "154906/154906 [==============================] - 61s 397us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 188/200\n",
            "154906/154906 [==============================] - 59s 381us/step - loss: 0.4003 - accuracy: 0.8236\n",
            "Epoch 189/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4001 - accuracy: 0.8239\n",
            "Epoch 190/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4000 - accuracy: 0.8238\n",
            "Epoch 191/200\n",
            "154906/154906 [==============================] - 60s 385us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 192/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4002 - accuracy: 0.8239\n",
            "Epoch 193/200\n",
            "154906/154906 [==============================] - 59s 378us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 194/200\n",
            "154906/154906 [==============================] - 59s 381us/step - loss: 0.4001 - accuracy: 0.8238\n",
            "Epoch 195/200\n",
            "154906/154906 [==============================] - 59s 382us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 196/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 197/200\n",
            "154906/154906 [==============================] - 61s 394us/step - loss: 0.4002 - accuracy: 0.8237\n",
            "Epoch 198/200\n",
            "154906/154906 [==============================] - 59s 380us/step - loss: 0.4002 - accuracy: 0.8238\n",
            "Epoch 199/200\n",
            "154906/154906 [==============================] - 59s 383us/step - loss: 0.4001 - accuracy: 0.8237\n",
            "Epoch 200/200\n",
            "154906/154906 [==============================] - 59s 379us/step - loss: 0.4001 - accuracy: 0.8238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd_5RwKgkWel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "190fab7a-33f6-4eae-bc06-3d56c1e5ff97"
      },
      "source": [
        "score1,acc1 = model.evaluate(X_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score1))\n",
        "print(\"acc: %.2f\" % (acc1))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1304d16275b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscore1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc: %.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1359\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                                          \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m                                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     def predict(self, x,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0mindices_for_conversion_to_dense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzSypvznqYtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "My Drive/kaggle_sentiment_data/test.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBvqllvQqGEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/kaggle_sentiment_data/model1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IenMVbA_qrHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f5eb756-4d80-4687-e584-ebd56a5600d9"
      },
      "source": [
        "predictions1 = model.predict_classes(X_test)\n",
        "print(predictions1[50])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WLictmSt5GU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "371e5cf9-26fd-4c94-f3df-844ccd72e94f"
      },
      "source": [
        "predictions1.size"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsHCdfIUuESB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f1a606f-96da-4fdd-defc-758801ca69f9"
      },
      "source": [
        "predictions1.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66292,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kGzJXKGqx71",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "1224a8aa-1dad-45fc-b1c2-23a72612e54d"
      },
      "source": [
        "predictions1_prob = model.predict_proba(X_test)\n",
        "print(predictions1_prob)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.03653471 0.19098471 0.5593346  0.17703134 0.03611458]\n",
            " [0.03653471 0.19098471 0.5593346  0.17703134 0.03611458]\n",
            " [0.03653471 0.19098471 0.5593346  0.17703134 0.03611458]\n",
            " ...\n",
            " [0.03653471 0.19098471 0.5593346  0.17703134 0.03611458]\n",
            " [0.03653475 0.19098493 0.55933523 0.17703153 0.03611352]\n",
            " [0.03653475 0.19098493 0.55933523 0.17703152 0.03611352]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBUSmUVAthG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}